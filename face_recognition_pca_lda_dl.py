# -*- coding: utf-8 -*-
"""face-recognition-pca-lda-dl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jTLNPlZlK-zCa0kUpZ1U43jdmbaN7puf

# **Face Recognition Using PCA & LDA**
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import KernelPCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

"""## Generate the Data Matrix and the Label vector

"""

paths = ["/kaggle/input/att-database-of-faces/s" + str(i) for i in range(1, 41)]
cnt = 0
Data = np.zeros((400, 10304))
labels = np.zeros((400, 1))
for i in range(40):
    labels[i * 10 : (i + 1) * 10] = i + 1
for path in paths:
    files = os.listdir(path)
    for file in files:
        img = Image.open(path + "/" + file)
        np_img = np.array(img)
        np_img = np_img.flatten()
        Data[cnt] = np_img
        cnt += 1

"""### Data Visualization

"""

image_height = 112
image_width = 92

fig, axs = plt.subplots(4, 10, figsize=(16, 10))

axs = axs.ravel()

for i in range(40):
    image_array = np.reshape(Data[(i) * 10], (image_height, image_width))
    axs[i].imshow(image_array, cmap="gray")
    axs[i].set_title(i + 1)
    axs[i].axis("off")

plt.tight_layout()
plt.show()

"""## Splitting The Dataset

"""

X_train = Data[0::2]
X_test = Data[1::2]
y_train = labels[0::2]
y_test = labels[1::2]
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""## PCA

"""

def get_PCA(training_data, alpha):
    mean_face = np.mean(training_data, axis=0)
    training_data_centralized = training_data - mean_face
    cov_matrix = training_data_centralized @ training_data_centralized.T
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    eigenvectors_converted = training_data_centralized.T @ eigenvectors
    eigenfaces = eigenvectors_converted / np.linalg.norm(eigenvectors_converted, axis=0)
    sum = 0
    no_components = 0
    for i in range(len(eigenvalues)):
        sum += eigenvalues[i]
        no_components += 1
        if sum / np.sum(eigenvalues) >= alpha:
            break
    return mean_face, eigenfaces[:, :no_components]


mean_face, eigenfaces = get_PCA(X_train, 0.8)
print(eigenfaces.shape)

"""## Projection training Data and Test data

"""

def PCA_Projected_data(training_data,testing_data,mean_face, eigenfaces):
    X_train_centered = training_data - mean_face
    X_train_projected = X_train_centered @ eigenfaces
    X_test_centered = testing_data - mean_face
    X_test_projected = X_test_centered @ eigenfaces
    return X_train_projected, X_test_projected

"""## Plotting The first 5 Eigen Faces

"""

_, eigenfaces = get_PCA(X_train, 0.95)
fig, axs = plt.subplots(1, 5, figsize=(16, 10))
for i in range(5):
    image_array = np.reshape(eigenfaces[:, i], (image_height, image_width))
    axs[i].imshow(image_array, cmap="gray")
    axs[i].set_title("Eigenface " + str(i + 1))
    axs[i].axis("off")

"""### Test PCA

"""

def Test_PCA(training_data,testing_data,training_labels,testing_labes,alpha, k):
    mean_face, eigenfaces = get_PCA(training_data, alpha)
    X_train_pca, X_test_pca = PCA_Projected_data(training_data,testing_data,mean_face, eigenfaces)
    knn = KNeighborsClassifier(k, weights="distance")
    knn.fit(X_train_pca, training_labels.ravel())
    y_pred = knn.predict(X_test_pca)
    accuracy = accuracy_score(testing_labes, y_pred.ravel())
    return accuracy


print("PCA Accuracy: " + str(Test_PCA(X_train,X_test,y_train,y_test,0.85, 1)))

"""## Classifier Tuning for PCA

"""

alphas = [0.8, 0.85, 0.9, 0.95]
k_values = [1, 3, 5, 7, 9]
df = pd.DataFrame(index=alphas, columns=k_values)
for num_dominant_eigen_vectors in alphas:
    for k in k_values:
        accuracy = Test_PCA(X_train,X_test,y_train,y_test,num_dominant_eigen_vectors, k)
        df.loc[num_dominant_eigen_vectors, k] = accuracy

df

for k in k_values:
    plt.plot(df.index, df[k], marker="o", label=f"k={k}")

plt.xlabel("Alpha")
plt.ylabel("Accuracy")
plt.title("Accuracy for different alphas and k values")
plt.legend(loc="upper right")
plt.grid(True)
plt.show()

"""## LDA

"""

def get_LDA(X_train, y_train):
    y_train = np.squeeze(y_train)
    class_means = np.array([np.mean(X_train[y_train == i], axis=0) for i in range(1, 41)])
    class_sizes = np.array([np.sum(y_train == i) for i in range(1, 41)])

    overall_mean = np.mean(X_train, axis=0)

    S_W = np.zeros((X_train.shape[1], X_train.shape[1]))
    for i in range(1, 41):
        class_data = X_train[y_train == i]
        centered_data = class_data - class_means[i - 1]
        S_W += np.dot(centered_data.T, centered_data)

    S_W += 1e-7 * np.identity(X_train.shape[1])

    S_B = np.zeros((X_train.shape[1], X_train.shape[1]))
    for i in range(1, 41):
        class_data = X_train[y_train == i]
        class_diff = class_means[i - 1] - overall_mean
        S_B += class_sizes[i - 1] * np.outer(class_diff, class_diff)

    eigenvalues, eigenvectors = np.linalg.eig(np.dot(np.linalg.inv(S_W), S_B))

    idx = np.argsort(eigenvalues)[::-1]
    sorted_eigenvectors = eigenvectors[:, idx]

    projection_matrix = sorted_eigenvectors[:, :39]
    return np.real(projection_matrix)

"""## The Projection Matrix of LDA

"""

LDA_projection_matrix = get_LDA(X_train,y_train)
print(LDA_projection_matrix.shape)

"""## LDA Projection

"""

def LDA_projected_data(training_data,test_data,projection_matrix):
    projected_X_train = np.dot(training_data, projection_matrix)
    projected_X_test = np.dot(test_data, projection_matrix)
    return projected_X_train, projected_X_test

"""### Test LDA

"""

def Test_LDA(k):
    projected_X_train, projected_X_test = LDA_projected_data(X_train,X_test,LDA_projection_matrix)
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(projected_X_train, y_train.ravel())
    y_pred = knn.predict(projected_X_test)
    accuracy = accuracy_score(y_test, y_pred.ravel())
    return accuracy


print("LDA Accuracy: " + str(Test_LDA(1)))

"""## Classifier Tuning for LDA

"""

k_values = [1, 3, 5, 7, 9]

results = []
projected_X_train, projected_X_test = LDA_projected_data(X_train,X_test,LDA_projection_matrix)
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k, weights="distance")
    knn.fit(projected_X_train, y_train.ravel())
    y_pred = knn.predict(projected_X_test)
    accuracy = accuracy_score(y_test, y_pred.ravel())
    results.append({"accuracy": accuracy})

df = pd.DataFrame(results, index=k_values)
df.index.name = "k"
df

plt.plot(df, marker="o")
plt.xlabel("K value")
plt.ylabel("Accuracy")
plt.title("Accuracy for different k values")
plt.grid(True)
plt.show()

"""## Comparison Between PCA & LDA

### Accuracy of PCA VS LDA with respect to K (number of nearest neighbours)
"""

alpha = [0.8, 0.85, 0.9, 0.95]
k_values = [1, 3, 5, 7, 9]
variants = [
    "PCA α = 0.8",
    "PCA α = 0.85",
    "PCA α = 0.9",
    "PCA α = 0.95",
    "LDA",
]
df = pd.DataFrame(index=variants, columns=k_values)
for num_dominant_eigen_vectors in alphas:
    for k in k_values:
        for i in range(4):
            pca_accuracy = Test_PCA(X_train,X_test,y_train,y_test,alpha[i], k)
            df.loc[variants[i], k] = str(pca_accuracy * 100) + "%"
        lda_accuracy = Test_LDA(k)
        df.loc["LDA", k] = str(lda_accuracy * 100) + "%"
df

"""## Splitting Data 70% Train and 30% Test"""

def split_data(data,labels):
    bonus_x_train = np.zeros((280,10304))
    bonus_x_test = np.zeros((120,10304))
    bonus_y_train = np.zeros((280,1))
    bonus_y_test = np.zeros((120,1))
    for  i in range (40):
        bonus_x_train[i*7:(i+1)*7] = data[i*10:i*10+7]
        bonus_x_test[i*3:(i+1)*3] = data[i*10+7:i*10+10]
        bonus_y_train[i*7:(i+1)*7] = labels[i*10:i*10+7]
        bonus_y_test[i*3:(i+1)*3] = labels[i*10+7:i*10+10]
    indices = np.arange(280)
    np.random.shuffle(indices)
    bonus_x_train = bonus_x_train[indices]
    bonus_y_train = bonus_y_train[indices]
    return bonus_x_train,bonus_x_test,bonus_y_train,bonus_y_test
bonus_x_train,bonus_x_test,bonus_y_train,bonus_y_test = split_data(Data,labels)

bonus_x_train, bonus_x_test, bonus_y_train, bonus_y_test = split_data(Data, labels)

mean_face, eigenfaces = get_PCA(bonus_x_train, 0.85)
X_train_pca, X_test_pca = PCA_Projected_data(bonus_x_train,bonus_x_test,mean_face, eigenfaces)
knn = KNeighborsClassifier(1, weights="distance")
knn.fit(X_train_pca, bonus_y_train.ravel())
y_pred = knn.predict(X_test_pca)
accuracy = accuracy_score(bonus_y_test, y_pred.ravel())
print("PCA Accuracy: " + str(accuracy))

"""### Comparison Between Ordinary Split and Train Split in PCA"""

alpha = [0.8, 0.85, 0.9, 0.95]
k=1
df = pd.DataFrame(index=alpha, columns=["Original Split", "Bonus Split"])
for num_dominant_eigen_vectors in alpha:
    pca_accuracy = Test_PCA(X_train,X_test,y_train,y_test,num_dominant_eigen_vectors, k)
    df.loc[num_dominant_eigen_vectors, "Original Split"] = pca_accuracy
    bonus_pca_accuracy = Test_PCA(bonus_x_train,bonus_x_test,bonus_y_train,bonus_y_test,num_dominant_eigen_vectors, k)
    df.loc[num_dominant_eigen_vectors, "Bonus Split"] = bonus_pca_accuracy
df

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
df.plot(kind="bar", ax=ax[0])
df.plot(kind="line", ax=ax[1], marker="o")
ax[0].set_title("Bar Chart")
ax[1].set_title("Line Chart")
plt.grid(True)
plt.show()

"""### Comparison Between Ordinary Split and Bonus Split in LDA"""

def get_acc(X_train, X_test, y_train, y_test, k):
    knn = KNeighborsClassifier(k, weights="distance")
    knn.fit(X_train, y_train.ravel())
    y_pred = knn.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred.ravel())
    return accuracy
Bonus_LDA_projection_matrix = get_LDA(bonus_x_train, bonus_y_train)
projected_X_train, projected_X_test = LDA_projected_data(bonus_x_train,bonus_x_test,LDA_projection_matrix)
acc_Lda_07 = get_acc(projected_X_train, projected_X_test, bonus_y_train, bonus_y_test, 1)
print("LDA Accuracy: " + str(acc_Lda_07))

df = pd.DataFrame(index=[], columns=["Original Split", "Bonus Split"])
df.loc["LDA", "Original Split"] = Test_LDA(1)
df.loc["LDA", "Bonus Split"] = acc_Lda_07
df

df.plot(kind="bar")
plt.title("LDA Accuracy")
plt.show()

"""# **Face regcognition using DL**"""

import numpy as np
import matplotlib.pyplot as plt
import os

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

dataset_path = '/kaggle/input/att-database-of-faces'
dataset_faces = []
for person in range(1, 41):
    temp = []
    for pose in range(1, 11):
        image_path = f'{dataset_path}/s{person}/{pose}.pgm'
        image = plt.imread(image_path)
        temp.append(image)
    dataset_faces.append(np.array(temp))
dataset_faces = np.array(dataset_faces)

print('Total number of datasets:', len(dataset_faces))
print('Dataset size:', dataset_faces.shape)

def plot_images(images, n=10):
    plt.figure(figsize=(15, 5))
    for i in range(n):
        plt.subplot(1, n, i+1)
        plt.imshow(images[i], cmap='gray')
        plt.axis('off')
    plt.show()

plot_images(dataset_faces[0])

num_subjects, num_images_per_subject, height, width = dataset_faces.shape
X = dataset_faces.reshape(num_subjects * num_images_per_subject, height, width, 1)
y = np.repeat(np.arange(num_subjects), num_images_per_subject)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)

X_train = X_train / 255.0
X_test = X_test / 255.0
X_val = X_val / 255.0

y_train = to_categorical(y_train, num_subjects)
y_test = to_categorical(y_test, num_subjects)
y_val = to_categorical(y_val, num_subjects)

model = Sequential([
    Conv2D(64, (3, 3), activation='relu', input_shape=(height, width, 1)),
    MaxPooling2D(2, 2),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Conv2D(256, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(num_subjects, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=64)

model.save('trained_model.keras')

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over Epochs')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss over Epochs')
plt.legend()
plt.show()

predictions = model.predict(X_test)

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print("Loss:", loss)
print("Accuracy:", accuracy)

predicted_labels = np.argmax(predictions, axis=1)

plt.figure(figsize=(15, 8))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(X_test[i].reshape(height, width), cmap='gray')
    plt.title(f'Predicted: {predicted_labels[i]}')
    plt.axis('off')
plt.show()